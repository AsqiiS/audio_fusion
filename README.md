# Audio_fusion
Project description can be found in report. 


## Training setup: 
The model is trained using train.py, where dataset and model parameters can be configured. 

## Evaluation: 
Text generation is evaluated on evaluate_text.py and audio is evaluated on evaluate_sim.py 

## Contributions 
This code is adapted from multiple papers and implementations, like: https://github.com/SWivid/F5-TTS, https://github.com/lucidrains/transfusion-pytorch, https://github.com/MonoFormer/MonoFormer/tree/main 
